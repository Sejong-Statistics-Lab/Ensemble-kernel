# 2023.11.09

## <Collaborative Work>

### 🌟Collaborative

## <Personal Work>

### ⛄Nayeon

- Ensemble Kernel cross validation
    
    코드 확인은 Ensemble-Kernel repositories
    
    - repetition 100번 실행
        - 어제했던 과정을 100번 실행 후 mean 확인
        
        ```python
        #all_results : 100번 돌린 모든 결과를 저장해둔 dataframe
        all_results=pd.DataFrame()
        
        all_results['train_C_index']=[]
        all_results['test_C_index']=[]
        all_results['remaining_variables']=[]
        
        import random
        
        with open("random_state_100.txt", "r") as file:
            random_state=file.read()
            
        random_state=random_state.split("\n")
        random_state=[int(x) for x in random_state if x]
        
        #최종 C-index 결과 확인
        for i in range(100):
            x_train, x_test, target_train, target_test = split_data(df_onehot, randomState = random_state[i])
        
            y_train = prepare_response_variable(target_train)
            y_test = prepare_response_variable(target_test)
        
            #os, status 분리
            drop = x_train[['Age','OS','Status']]
            x_train_drop = x_train.drop(columns=['Age','OS','Status'])
        
            # 데이터프레임의 열 이름을 리스트로 저장
            columns = x_train_drop.columns
        
            column_groups = []
        
            for i in range(len(columns)):
                # i+1개씩 선택한 모든 열 조합을 생성
                all_column_combinations = list(combinations(columns, i+1))
        
                # 각 조합에 대한 데이터프레임을 생성하고 출력
                for column_combination in all_column_combinations:
                    selected_columns = list(column_combination)
                    sub_train1 = x_train_drop[selected_columns]
                    sub_train2 = pd.concat([sub_train1, drop], axis=1)
                    column_groups.append(sub_train2)
            
            num_groups = 3
            best_cindex = []
        
            for i in range(len(column_groups)):
                x_groups = []
                train = column_groups[i].sample(frac=1).reset_index(drop=True)
        
                # train data를 그룹으로 나누기
                for i in range(num_groups):
                    censored = train[train['Status'] == 0]  # 0인 행만 추출
                    uncensored = train[train['Status'] == 1]  # 1인 행만 추출
        
                    group_size1 = len(censored) // num_groups
                    group_size2 = len(uncensored) // num_groups
        
                    if i < num_groups - 1:
                        #censoring 비율 맞추기
                        group1 = censored.iloc[i * group_size1:(i + 1) * group_size1]
                        group2 = uncensored.iloc[i * group_size2:(i + 1) * group_size2]
        
                        group = pd.concat([group1, group2], ignore_index=True)
                    else:
                        group1 = censored.iloc[i * group_size1:]
                        group2 = uncensored.iloc[i * group_size2:]
        
                        group = pd.concat([group1, group2], ignore_index=True)
            
                    x_groups.append(group)
                
                cindex=[]
        
                for i in range(len(x_groups)):
        
                    temp = []
                    for j in range(len(x_groups)):
                        if i != j:
                            temp.append(x_groups[j])
                    #train = train data(등분한 것 중 1개를 제외한 나머지)
                    train=pd.concat(temp)
        
                    #validation = validation data(등분한 것 중 1개)
                    validation = x_groups[i]
        
                    train_target=train[['Status','OS']]
                    test_target=validation[['Status','OS']]
        
                    train_y=prepare_response_variable(train_target)
                    test_y=prepare_response_variable(test_target)
                    #data정의 -> train, validation, train_y, test_y
        
                    result = C_index_fastKernelSurvivalSVM(train, train_y, validation, test_y, param_grid, keywords = ['Age'], drop=False, coef_drop=None)
                
                    cindex.append(result[1])
                    #print(result[1]) #생략 가능
                
                best_cindex.append(np.mean(cindex))
        
            #best_cindex : 선택된 변수별 모든 validation cindex값을 list로 저장
            #max_num : best cindex의 리스트 번호
            max_num = best_cindex.index(max(best_cindex))
        
            #train_column : best cindex의 선택된 변수
            train_column = column_groups[max_num].columns
        
            #선택된 변수만을 가지고 train, test 재정의
            #x_train_re, y_train, x_test_re, y_test
            x_train_re=x_train[train_column]
            x_test_re=x_test[train_column]
        
            #result_ensemble : train cindex, test cindex, remaining variables 순서대로 결과가 저장된 리스트
            result_ensemble = C_index_fastKernelSurvivalSVM(x_train_re, y_train, x_test_re, y_test, param_grid, keywords = ['Age', 'Sex'], drop=False, coef_drop=None)
            
            all_results=all_results.append({"train_C_index":result_ensemble[0],"test_C_index":result_ensemble[1],"remaining_variables":result_ensemble[2]}, ignore_index=True)
        
            all_results.to_csv("veteran_cox_repetition.csv", index = False, encoding = 'cp949')
        
        ```
        

### 💪🏻Beomseok